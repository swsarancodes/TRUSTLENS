{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5256696,"sourceType":"datasetVersion","datasetId":3041726}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"dataset_dir = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/\" \nprint(\"Loading dataset from: \" + dataset_dir)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T14:32:40.865944Z","iopub.execute_input":"2025-10-30T14:32:40.866129Z","iopub.status.idle":"2025-10-30T14:32:40.872991Z","shell.execute_reply.started":"2025-10-30T14:32:40.866112Z","shell.execute_reply":"2025-10-30T14:32:40.872378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nfrom PIL import Image\nfrom pathlib import Path\nfrom timeit import default_timer as Timer\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import Subset, DataLoader, ConcatDataset, Dataset\nfrom torchinfo import summary\nimport onnx\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n\n\nprint(torch.__version__)\nprint(torchvision.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T14:32:45.253136Z","iopub.execute_input":"2025-10-30T14:32:45.253632Z","iopub.status.idle":"2025-10-30T14:32:53.306138Z","shell.execute_reply.started":"2025-10-30T14:32:45.253598Z","shell.execute_reply":"2025-10-30T14:32:53.304974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T14:32:55.541331Z","iopub.execute_input":"2025-10-30T14:32:55.541700Z","iopub.status.idle":"2025-10-30T14:32:55.631065Z","shell.execute_reply.started":"2025-10-30T14:32:55.541675Z","shell.execute_reply":"2025-10-30T14:32:55.630489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_path = dataset_dir + \"train\"\ntest_path = dataset_dir + \"test\"\ntrain_path, test_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T14:32:57.471158Z","iopub.execute_input":"2025-10-30T14:32:57.471567Z","iopub.status.idle":"2025-10-30T14:32:57.477936Z","shell.execute_reply.started":"2025-10-30T14:32:57.471533Z","shell.execute_reply":"2025-10-30T14:32:57.477268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.RandomApply([\n        transforms.GaussianBlur(kernel_size = 3, sigma = (0.1,0.3))\n    ], p = 0.5),\n    transforms.Resize((32,32)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n                        std = [0.229, 0.224, 0.225])\n])\ntest_transform = transforms.Compose([\n    transforms.Resize((32,32)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n                        std = [0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T14:32:59.991618Z","iopub.execute_input":"2025-10-30T14:32:59.991896Z","iopub.status.idle":"2025-10-30T14:32:59.997414Z","shell.execute_reply.started":"2025-10-30T14:32:59.991876Z","shell.execute_reply":"2025-10-30T14:32:59.996733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = datasets.ImageFolder(root = train_path,\n                                 transform = train_transform,\n                                 target_transform = None)\ntest_data = datasets.ImageFolder(root = test_path,\n                                transform = test_transform)\nlen(train_data), len(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T14:33:03.331514Z","iopub.execute_input":"2025-10-30T14:33:03.331784Z","iopub.status.idle":"2025-10-30T14:36:36.410730Z","shell.execute_reply.started":"2025-10-30T14:33:03.331764Z","shell.execute_reply":"2025-10-30T14:36:36.410114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = [\"FAKE\", \"REAL\"]\n\nrandom_idx = random.randint(0, 100000)\nplt.imshow(train_data[random_idx][0].permute(1,2,0))\nplt.title(f\"Image class: {class_names[train_data[random_idx][1]]} & Image shape : {train_data[random_idx][0].permute(1,2,0).shape}\")\nplt.axis(False);","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T14:36:55.819296Z","iopub.execute_input":"2025-10-30T14:36:55.820009Z","iopub.status.idle":"2025-10-30T14:36:56.202103Z","shell.execute_reply.started":"2025-10-30T14:36:55.819982Z","shell.execute_reply":"2025-10-30T14:36:56.201464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 32\ntrain_dataloaders = DataLoader(train_data,\n                              BATCH_SIZE,\n                              shuffle = True)\ntest_dataloaders = DataLoader(test_data,\n                             BATCH_SIZE)\ntrain_dataloaders, test_dataloaders","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T14:37:40.316450Z","iopub.execute_input":"2025-10-30T14:37:40.316968Z","iopub.status.idle":"2025-10-30T14:37:40.322273Z","shell.execute_reply.started":"2025-10-30T14:37:40.316946Z","shell.execute_reply":"2025-10-30T14:37:40.321732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure(figsize = (8,8))\n\nfig.suptitle(\"Batch Images\", fontsize=32)\nrows, columns = 4, 8\nfor batch_idx, (img, label) in enumerate(train_dataloaders):\n    if (batch_idx < 1):\n        for i in range(1, rows * columns + 1):\n            fig.add_subplot(rows, columns, i)\n            plt.imshow(img[i-1].permute(1,2,0))\n            plt.title(class_names[int(label[i-1])], fontsize = 12)\n            plt.axis(False);","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T14:37:43.431620Z","iopub.execute_input":"2025-10-30T14:37:43.431899Z","iopub.status.idle":"2025-10-30T14:43:18.562221Z","shell.execute_reply.started":"2025-10-30T14:37:43.431877Z","shell.execute_reply":"2025-10-30T14:43:18.560995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CNNBlock(nn.Module):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.input_shape = input_shape\n        self.Layer = torch.nn.Sequential(\n            nn.Conv2d(input_shape, hidden_units, kernel_size = (3,3)),\n            nn.BatchNorm2d(hidden_units),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, kernel_size = (3,3)),\n            nn.BatchNorm2d(hidden_units),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, output_shape, kernel_size = (5,5)),\n            nn.BatchNorm2d(output_shape),\n            nn.ReLU()\n        )\n    def get_output_shape(self, input_height, input_width):\n        x = torch.randn(1, self.input_shape, input_height, input_width)\n        return self.Layer(x).shape[2:]\n\n    def forward(self, x):\n        return self.Layer(x)\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self,\n                 in_channels: int,\n                 patch_size: int,\n                 embedding_dim: int) -> None:\n        super().__init__()\n        self.in_channels = in_channels\n        self.patch_size = patch_size\n        self.embedding_dim = embedding_dim\n        self.patcher = nn.Conv2d(in_channels= in_channels,\n                                 out_channels= embedding_dim,\n                                 stride= patch_size,\n                                 kernel_size= patch_size,\n                                 padding= 0)\n        self.flatten = nn.Flatten(start_dim= 2,\n                                  end_dim= 3)\n\n    def forward(self, x):\n        image_res = x.shape[-1]\n        x_patched = self.patcher(x)\n        x_flattened = self.flatten(x_patched)\n        return x_flattened.permute(0,2,1)\n    \nclass MultiHeadSelfAttentionBlock(nn.Module):\n    def __init__(self,\n                 embedding_dim : int,\n                 num_heads : int,\n                 att_dropout : float):\n        super().__init__()\n\n        self.LayerNorm = nn.LayerNorm(normalized_shape= embedding_dim)\n\n        self.MultiHeadAttention = nn.MultiheadAttention(embed_dim= embedding_dim,\n                                                        num_heads= num_heads,\n                                                        dropout= att_dropout,\n                                                        batch_first= True)\n\n    def forward(self, x):\n        x = self.LayerNorm(x)\n        attn_output, _ = self.MultiHeadAttention(query= x,\n                                                 key= x,\n                                                 value= x,\n                                                 need_weights = False)\n        return attn_output\n    \nclass MultiLayerPreceptronBlock(nn.Module):\n    def __init__(self,\n                 embedding_dim: int,\n                 mlp_size: int,\n                 dropout: float):\n        super().__init__()\n\n        self.LayerNorm = nn.LayerNorm(normalized_shape= embedding_dim)\n\n        self.MLP = nn.Sequential(\n            nn.Linear(in_features= embedding_dim,\n                      out_features= mlp_size),\n            nn.GELU(),\n            nn.Dropout(p=dropout),\n            nn.Linear(in_features= mlp_size,\n                      out_features= embedding_dim),\n            nn.Dropout(p= dropout)\n        )\n\n    def forward(self, x):\n        x = self.LayerNorm(x)\n        x = self.MLP(x)\n        return x\n    \nclass TransformerEncoder(nn.Module):\n    def __init__(self,\n                 embedding_dim: int,\n                 num_heads: int,\n                 mlp_size: int,\n                 attn_dropout: float,\n                 mlp_dropout: float):\n        super().__init__()\n        self.MSA_Block = MultiHeadSelfAttentionBlock(embedding_dim= embedding_dim,\n                                               num_heads= num_heads,\n                                               att_dropout= attn_dropout)\n        self.MLP_Block = MultiLayerPreceptronBlock(embedding_dim= embedding_dim,\n                                             mlp_size= mlp_size,\n                                             dropout= mlp_dropout)\n\n    def forward(self, x):\n        x = self.MSA_Block(x) + x\n        x = self.MLP_Block(x) + x\n        x = self.MSA_Block(x) + x\n        return x\n    \nclass ViTBlock(nn.Module):\n    def __init__(self,\n                 image_size: int,\n                 in_channels: int,\n                 patch_size: int,\n                 num_transformer_layers: int,\n                 embedding_dim: int,\n                 mlp_size: int,\n                 num_heads: int,\n                 attn_dropout: float,\n                 mlp_dropout: float,\n                 embedding_dropout: float,\n                 num_classes: int = 2):\n        super().__init__()\n\n        self.num_patches = (image_size // patch_size) ** 2\n\n        self.class_embedding = nn.Parameter(torch.randn(1, 1, embedding_dim),\n                                            requires_grad= True)\n\n        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embedding_dim),\n                                               requires_grad= True)\n\n        self.patch_embedding = PatchEmbedding(in_channels= in_channels,\n                                              patch_size= patch_size,\n                                              embedding_dim= embedding_dim)\n\n        self.embedding_dropout = nn.Dropout(p = embedding_dropout)\n\n        self.transformerencoder = nn.Sequential(* [TransformerEncoder(embedding_dim= embedding_dim,\n                                                     num_heads= num_heads,\n                                                     mlp_size= mlp_size,\n                                                     attn_dropout= attn_dropout,\n                                                     mlp_dropout= mlp_dropout) for _ in range(num_transformer_layers)])\n\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n\n        class_token = self.class_embedding.expand(batch_size, -1, -1)\n\n        x = self.patch_embedding(x)\n\n        x = torch.cat((class_token, x), dim = 1)\n\n        x = self.position_embedding + x\n\n        x = self.embedding_dropout(x)\n\n        x = self.transformerencoder(x)\n\n        return x\n    \nclass AttentionMechBlock(nn.Module):\n    def __init__(self, dim, units=128):\n        super().__init__()\n        self.query = nn.Linear(dim, units)\n        self.key = nn.Linear(dim, units)\n        self.value = nn.Linear(dim, units)\n        self.LayerNorm = nn.LayerNorm(normalized_shape= units)\n\n    def forward(self, x):\n        Q = self.query(x)\n        K = self.key(x)\n        V = self.value(x)\n        attn = torch.softmax(Q @ K.transpose(1,2) / (x.size(-1)**0.5), dim=-1)\n        return self.LayerNorm((attn @ V).mean(dim=1))\n\nclass HybridModel(nn.Module):\n\n    def __init__(self,\n                 image_size: int,\n                 in_channels: int,\n                 hidden_units: int,\n                 output_shape: int,\n                 patch_size: int,\n                 num_transformer_layers: int,\n                 embedding_dim: int,\n                 mlp_size: int,\n                 num_heads: int,\n                 attn_dropout: float,\n                 mlp_dropout: float,\n                 embedding_dropout: float,\n                 units: int = 128,\n                 num_classes: int = 2):\n        super().__init__()\n        self.CNNBlock = CNNBlock(input_shape= 3,\n                                 hidden_units= hidden_units,\n                                 output_shape= output_shape)\n        self.cnn_output_height, self.cnn_output_width = self.CNNBlock.get_output_shape(image_size, image_size)\n        self.ViTBlock = ViTBlock(image_size= self.cnn_output_height,\n                                 in_channels= in_channels,\n                                 patch_size= patch_size,\n                                 num_transformer_layers= num_transformer_layers,\n                                 embedding_dim= embedding_dim,\n                                 mlp_size= mlp_size,\n                                 num_heads= num_heads,\n                                 attn_dropout= attn_dropout,\n                                 mlp_dropout= mlp_dropout,\n                                 embedding_dropout= embedding_dropout,\n                                 num_classes= num_classes)\n        self.AttentionMechBlock = AttentionMechBlock(dim= embedding_dim,\n                                                     units= units)\n        self.classifier = torch.nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(p = mlp_dropout),\n            nn.Linear(in_features= units,\n                      out_features= num_classes)\n        )\n\n    def forward(self, x):\n        x = self.CNNBlock(x)\n        x = self.ViTBlock(x)\n        x = self.AttentionMechBlock(x)\n        x = self.classifier(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T14:43:37.162543Z","iopub.execute_input":"2025-10-30T14:43:37.163251Z","iopub.status.idle":"2025-10-30T14:43:37.184655Z","shell.execute_reply.started":"2025-10-30T14:43:37.163227Z","shell.execute_reply":"2025-10-30T14:43:37.184114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Hybrid_Model = HybridModel(image_size = 32,\n                          in_channels = 64,\n                          hidden_units = 32,\n                          output_shape = 64,\n                          patch_size = 5,\n                          num_transformer_layers = 5,\n                          embedding_dim = 256,\n                          mlp_size = 2048,\n                          num_heads = 128,\n                          attn_dropout = 0.1,\n                          mlp_dropout = 0,\n                          embedding_dropout = 0,\n                          units = 128,\n                          num_classes = 2).to(device)\nHybrid_Model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T14:43:44.551117Z","iopub.execute_input":"2025-10-30T14:43:44.551384Z","iopub.status.idle":"2025-10-30T14:43:44.883737Z","shell.execute_reply.started":"2025-10-30T14:43:44.551365Z","shell.execute_reply":"2025-10-30T14:43:44.883122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchinfo\ntorchinfo.summary(model = Hybrid_Model,\n                 input_size = (32,3,32,32),\n                 col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T14:43:50.515191Z","iopub.execute_input":"2025-10-30T14:43:50.515891Z","iopub.status.idle":"2025-10-30T14:43:51.478763Z","shell.execute_reply.started":"2025-10-30T14:43:50.515864Z","shell.execute_reply":"2025-10-30T14:43:51.478059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.Adam(Hybrid_Model.parameters(), lr = 0.0001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer = optimizer,\n    mode = \"min\",\n    factor = 0.5,\n    patience = 5,\n    verbose = True\n)\nlrs = []\nloss_func = torch.nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T14:43:59.384829Z","iopub.execute_input":"2025-10-30T14:43:59.385093Z","iopub.status.idle":"2025-10-30T14:43:59.391247Z","shell.execute_reply.started":"2025-10-30T14:43:59.385075Z","shell.execute_reply":"2025-10-30T14:43:59.390581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\nimport torch\n\n# Create directories for saving models\ncheckpoint_dir = \"/kaggle/working/checkpoints\"\nweights_dir = \"/kaggle/working/weights\"\nos.makedirs(checkpoint_dir, exist_ok=True)\nos.makedirs(weights_dir, exist_ok=True)\n\nepochs = 10\n\nresults = {\n    \"train_loss\": [],\n    \"train_accuracy\": [],\n    \"test_loss\": [],\n    \"test_accuracy\": []\n}\n\nfor epoch in tqdm(range(epochs)):\n    Hybrid_Model.train()\n    train_loss, train_acc = 0, 0\n    y_train_actual, y_train_predicted = [], []\n\n    for batch_idx, (x, y) in enumerate(train_dataloaders):\n        x, y = x.to(device), y.to(device)\n        y_pred = Hybrid_Model(x)\n        loss = loss_func(y_pred, y)\n        train_loss += loss.item()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        y_class_pred = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += (y_class_pred == y).sum().item() / len(y_pred)\n        y_train_actual.extend(y.cpu().numpy())\n        y_train_predicted.extend(y_class_pred.cpu().numpy())\n\n    train_loss /= len(train_dataloaders)\n    train_acc /= len(train_dataloaders)\n\n    Hybrid_Model.eval()\n    test_loss, test_acc = 0, 0\n    y_test_actual, y_test_predicted = [], []\n\n    with torch.inference_mode():\n        for batch_idx, (x, y) in enumerate(test_dataloaders):\n            x, y = x.to(device), y.to(device)\n            y_pred = Hybrid_Model(x)\n            loss = loss_func(y_pred, y)\n            test_loss += loss.item()\n\n            y_class_pred = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n            test_acc += (y_class_pred == y).sum().item() / len(y_pred)\n            y_test_predicted.extend(y_class_pred.cpu().numpy())\n            y_test_actual.extend(y.cpu().numpy())\n\n    test_loss /= len(test_dataloaders)\n    test_acc /= len(test_dataloaders)\n\n    # Store results\n    results[\"train_loss\"].append(train_loss)\n    results[\"train_accuracy\"].append(train_acc)\n    results[\"test_loss\"].append(test_loss)\n    results[\"test_accuracy\"].append(test_acc)\n\n    # Get current learning rate and step scheduler\n    curr_lr = optimizer.param_groups[0][\"lr\"]\n    scheduler.step(test_loss)\n\n    print(f\"Epoch {epoch + 1}/{epochs} | LR {curr_lr:.6f} | \"\n          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n          f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n\n    # Save checkpoint every 5 epochs\n    if (epoch + 1) % 5 == 0:\n        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': Hybrid_Model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'train_loss': train_loss,\n            'test_loss': test_loss,\n            'train_accuracy': train_acc,\n            'test_accuracy': test_acc,\n            'results': results\n        }, checkpoint_path)\n        print(f\"✅ Checkpoint saved at {checkpoint_path}\")\n\n# Save final model weights\nfinal_weights_path = os.path.join(weights_dir, \"final_weights.pth\")\ntorch.save(Hybrid_Model.state_dict(), final_weights_path)\nprint(f\"🎯 Final model weights saved at {final_weights_path}\")\n\nprint(\"✅ Model Training Completed\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T15:08:54.275001Z","iopub.execute_input":"2025-10-30T15:08:54.275262Z","iopub.status.idle":"2025-10-30T16:14:20.650453Z","shell.execute_reply.started":"2025-10-30T15:08:54.275243Z","shell.execute_reply":"2025-10-30T16:14:20.649609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classification_report(y_train_actual, y_train_predicted)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T16:15:48.509432Z","iopub.execute_input":"2025-10-30T16:15:48.509744Z","iopub.status.idle":"2025-10-30T16:15:48.674252Z","shell.execute_reply.started":"2025-10-30T16:15:48.509723Z","shell.execute_reply":"2025-10-30T16:15:48.673431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classification_report(y_test_actual, y_test_predicted)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T16:15:54.955174Z","iopub.execute_input":"2025-10-30T16:15:54.955794Z","iopub.status.idle":"2025-10-30T16:15:54.982052Z","shell.execute_reply.started":"2025-10-30T16:15:54.955767Z","shell.execute_reply":"2025-10-30T16:15:54.981265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(y_train_actual, y_train_predicted)\ndisp = ConfusionMatrixDisplay(cm, display_labels = class_names)\ndisp.plot();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T16:15:58.860755Z","iopub.execute_input":"2025-10-30T16:15:58.861391Z","iopub.status.idle":"2025-10-30T16:15:59.113183Z","shell.execute_reply.started":"2025-10-30T16:15:58.861366Z","shell.execute_reply":"2025-10-30T16:15:59.112476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(y_test_actual, y_test_predicted)\ndisp = ConfusionMatrixDisplay(cm, display_labels = class_names)\ndisp.plot();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T16:16:02.878066Z","iopub.execute_input":"2025-10-30T16:16:02.878364Z","iopub.status.idle":"2025-10-30T16:16:03.279227Z","shell.execute_reply.started":"2025-10-30T16:16:02.878342Z","shell.execute_reply":"2025-10-30T16:16:03.278365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = range(len(results[\"train_loss\"]))\nplt.figure(figsize=(15, 7))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs, results[\"train_loss\"], label=\"train loss\")\nplt.plot(epochs, results[\"test_loss\"], label=\"test loss\")\nplt.title(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs, results[\"train_accuracy\"], label=\"train accuracy\")\nplt.plot(epochs, results[\"test_accuracy\"], label=\"test accuracy\")\nplt.title(\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T16:17:24.497813Z","iopub.execute_input":"2025-10-30T16:17:24.498520Z","iopub.status.idle":"2025-10-30T16:17:24.914568Z","shell.execute_reply.started":"2025-10-30T16:17:24.498494Z","shell.execute_reply":"2025-10-30T16:17:24.913776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.onnx\n\n# Set model to evaluation mode\nHybrid_Model.eval()\n\n# Create output directory\nonnx_dir = \"/kaggle/working/onnx\"\nos.makedirs(onnx_dir, exist_ok=True)\n\n# Define the full path for the ONNX model\nonnx_path = os.path.join(onnx_dir, \"hybrid_model.onnx\")\n\n# Create dummy input tensor (batch_size, channels, height, width)\ndummy_input = torch.randn(1, 3, 32, 32, device=device, requires_grad=False)\n\n# Export the model\ntorch.onnx.export(\n    Hybrid_Model,                           # model being run\n    dummy_input,                            # model input (or a tuple for multiple inputs)\n    onnx_path,                              # where to save the model\n    export_params=True,                     # store the trained parameter weights inside the model file\n    opset_version=14,                       # the ONNX version to export the model to (14 is widely supported)\n    do_constant_folding=True,               # whether to execute constant folding for optimization\n    input_names=['input'],                  # the model's input names\n    output_names=['output'],                # the model's output names\n    dynamic_axes={\n        'input': {0: 'batch_size'},         # variable batch size\n        'output': {0: 'batch_size'}\n    }\n)\n\nprint(f\"✅ Model successfully exported to ONNX format\")\nprint(f\"📁 ONNX model saved at: {onnx_path}\")\n\n# Verify the ONNX model\ntry:\n    import onnx\n    onnx_model = onnx.load(onnx_path)\n    onnx.checker.check_model(onnx_model)\n    print(\"✅ ONNX model verification passed\")\nexcept ImportError:\n    print(\"⚠️ Install onnx package to verify the exported model: pip install onnx\")\nexcept Exception as e:\n    print(f\"⚠️ ONNX model verification failed: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T16:22:31.080463Z","iopub.execute_input":"2025-10-30T16:22:31.081173Z","iopub.status.idle":"2025-10-30T16:22:32.399013Z","shell.execute_reply.started":"2025-10-30T16:22:31.081148Z","shell.execute_reply":"2025-10-30T16:22:32.398282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n# Path to the image\nimage_path = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/test/FAKE/0 (7).jpg\"\n\n# Define class names (adjust based on your dataset)\nclass_names = ['REAL', 'FAKE']\n\n# Set model to evaluation mode\nHybrid_Model.eval()\n\n# Define the same transforms used during training\ntransform = transforms.Compose([\n    transforms.Resize((32, 32)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Load and preprocess the image\ndef predict_single_image(image_path, model, transform, device):\n    image = Image.open(image_path).convert('RGB')\n    original_image = image.copy()\n    image_tensor = transform(image)\n    image_tensor = image_tensor.unsqueeze(0)\n    image_tensor = image_tensor.to(device)\n    \n    with torch.inference_mode():\n        output = model(image_tensor)\n        probabilities = torch.softmax(output, dim=1)\n        predicted_class = torch.argmax(probabilities, dim=1).item()\n        confidence = probabilities[0][predicted_class].item()\n    \n    return original_image, predicted_class, confidence, probabilities[0]\n\n# Make prediction\noriginal_img, predicted_class, confidence, probs = predict_single_image(\n    image_path, Hybrid_Model, transform, device\n)\n\n# Display results\nplt.figure(figsize=(10, 5))\n\n# Show image\nplt.subplot(1, 2, 1)\nplt.imshow(original_img)\nplt.axis('off')\nplt.title(f\"Predicted: {class_names[predicted_class]} | Confidence: {confidence:.2%}\")\n\n# Show probability distribution\nplt.subplot(1, 2, 2)\nprobs_numpy = probs.cpu().numpy()\nbars = plt.bar(class_names, probs_numpy)\nbars[predicted_class].set_color('green')\nplt.ylabel('Probability')\nplt.title('Class Probabilities')\nplt.ylim([0, 1])\n\n# Add percentage labels on bars\nfor i, (name, prob) in enumerate(zip(class_names, probs_numpy)):\n    plt.text(i, prob + 0.02, f'{prob:.2%}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n# Print detailed results\nprint(\"=\" * 50)\nprint(f\"Image Path: {image_path}\")\nprint(f\"Predicted Class: {class_names[predicted_class]}\")\nprint(f\"Confidence: {confidence:.4f} ({confidence:.2%})\")\nprint(\"=\" * 50)\nprint(\"Class Probabilities:\")\nfor i, name in enumerate(class_names):\n    print(f\"  {name}: {probs[i].item():.4f} ({probs[i].item():.2%})\")\nprint(\"=\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T16:34:51.054730Z","iopub.execute_input":"2025-10-30T16:34:51.055203Z","iopub.status.idle":"2025-10-30T16:34:51.260109Z","shell.execute_reply.started":"2025-10-30T16:34:51.055179Z","shell.execute_reply":"2025-10-30T16:34:51.259423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}